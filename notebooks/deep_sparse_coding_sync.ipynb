{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "current_path = !pwd\n",
    "parent_path = os.path.dirname(current_path[0])\n",
    "if parent_path not in sys.path: sys.path.append(parent_path)\n",
    "work_path = \"/\".join(parent_path.split('/')[:-1])\n",
    "if work_path not in sys.path: sys.path.append(work_path)\n",
    "    \n",
    "import DeepSparseCoding as dsc\n",
    "from DeepSparseCoding.data.dataset import Dataset\n",
    "from DeepSparseCoding.analysis import analysis_picker as ap\n",
    "from DeepSparseCoding.utils import data_processing as dp\n",
    "import utils.model_handling as model_funcs\n",
    "import utils.dataset_generation as iso_data\n",
    "import utils.histogram_analysis as hist_funcs\n",
    "import utils.plotting as pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_width = 416.83269 #pt = 14.65cm\n",
    "fontsize = 12\n",
    "dpi = 800\n",
    "font_settings = {\n",
    "        \"text.usetex\": True,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"axes.labelsize\": fontsize,\n",
    "        \"axes.titlesize\": fontsize,\n",
    "        \"figure.titlesize\": fontsize,\n",
    "        \"font.size\": fontsize,\n",
    "        \"legend.fontsize\": fontsize,\n",
    "        \"xtick.labelsize\": fontsize-2,\n",
    "        \"ytick.labelsize\": fontsize-2,\n",
    "}\n",
    "mpl.rcParams.update(font_settings)\n",
    "mpl.pyplot.rc('text', usetex=True)\n",
    "\n",
    "color_vals = dict(zip([\"blk\", \"lt_green\", \"md_green\", \"dk_green\", \"lt_blue\", \"md_blue\", \"dk_blue\", \"lt_red\", \"md_red\", \"dk_red\"],\n",
    "  [\"#000000\", \"#A9DFBF\", \"#196F3D\", \"#27AE60\", \"#AED6F1\", \"#3498DB\", \"#21618C\", \"#F5B7B1\", \"#E74C3C\", \"#943126\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dsc_activations_cell(analyzer, images, neuron, batch_size=10, activation_operation=None):\n",
    "    \"\"\"\n",
    "    Returns the activations from a model for given input images\n",
    "    Parameters:\n",
    "        analyzer [DSC analyzer object] an object from the DeepSparseCoding library\n",
    "        images [np.ndarray] of size NumImages x W x H\n",
    "        neuron [int or vector of ints] that points to the neuron index\n",
    "        batch_size [int] specifying the batch size to use for the getting the neuron activations\n",
    "        activation_operation [function] to be used if the DSC model has a unique function handle for getting neuron activations (e.g. in the case of lca_subspace)\n",
    "    Output:\n",
    "        activations [np.ndarray] vector of length len(neuron)\n",
    "    \"\"\"\n",
    "    images = dp.reshape_data(images[..., None], flatten=analyzer.model.params.vectorize_data)[0]\n",
    "    activations = analyzer.compute_activations(images, batch_size, activation_operation)[:, neuron]\n",
    "    return activations\n",
    "\n",
    "def load_analysis(params):\n",
    "    params.model_dir = (work_path+\"/Projects/\"+params.model_name)\n",
    "    analyzer = ap.get_analyzer(params.model_type)\n",
    "    analyzer.setup(params)\n",
    "    analyzer.model.setup(analyzer.model_params)\n",
    "    analyzer.load_analysis(save_info=params.save_info)\n",
    "    return params, analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained LCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lca_1024_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_1024_vh\"\n",
    "    self.display_name = \"Sparse Coding\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "    self.use_group_activations = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, analyzer = load_analysis(lca_1024_vh_params())\n",
    "neuron_weights = [analyzer.bf_stats[\"basis_functions\"][idx]\n",
    "    for idx in range(len(analyzer.bf_stats[\"basis_functions\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_analysis = dict()\n",
    "\n",
    "#cont_analysis[\"target_neuron_ids\"] = get_rand_target_neuron_ids(3, len(meis[\"images\"]))\n",
    "cont_analysis[\"target_neuron_ids\"] = [992, 381, 62]# [1, 2, 3]\n",
    "cont_analysis[\"num_comparisons\"] = 3\n",
    "cont_analysis[\"min_angle\"] = 5\n",
    "cont_analysis[\"x_range\"] = (-2.0, 2.0)\n",
    "cont_analysis[\"y_range\"] = (-2.0, 2.0)\n",
    "cont_analysis[\"num_images\"] = int(30**2)\n",
    "cont_analysis[\"image_scale\"] = 33 # norm of training data\n",
    "\n",
    "iso_vectors = iso_data.compute_rand_vectors(\n",
    "    [neuron_weights[idx] for idx in cont_analysis[\"target_neuron_ids\"]],\n",
    "    cont_analysis[\"num_comparisons\"])\n",
    "cont_analysis[\"rand_target_vectors\"] = iso_vectors[0]\n",
    "cont_analysis[\"rand_orth_vectors\"] = iso_vectors[1]\n",
    "\n",
    "iso_vectors = iso_data.compute_comp_vectors(\n",
    "    neuron_weights,\n",
    "    cont_analysis[\"target_neuron_ids\"],\n",
    "    cont_analysis[\"min_angle\"],\n",
    "    cont_analysis[\"num_comparisons\"])\n",
    "cont_analysis[\"comparison_neuron_ids\"] = iso_vectors[0]\n",
    "cont_analysis[\"comparison_target_vectors\"] = iso_vectors[1]\n",
    "cont_analysis[\"comparison_vectors\"] = iso_vectors[2]\n",
    "\n",
    "cont_analysis[\"target_vectors\"] = cont_analysis[\"comparison_target_vectors\"]\n",
    "\n",
    "cont_analysis[\"contour_dataset\"], datapoints = iso_data.get_contour_dataset(\n",
    "    cont_analysis[\"target_vectors\"],\n",
    "    cont_analysis[\"comparison_vectors\"],\n",
    "    cont_analysis[\"x_range\"],\n",
    "    cont_analysis[\"y_range\"],\n",
    "    cont_analysis[\"num_images\"],\n",
    "    cont_analysis[\"image_scale\"])\n",
    "\n",
    "activation_function_kwargs = {\n",
    "    \"batch_size\": 100}\n",
    "\n",
    "cont_analysis[\"activations\"] = model_funcs.get_normalized_activations(\n",
    "    analyzer,\n",
    "    cont_analysis[\"target_neuron_ids\"],\n",
    "    datapoints,\n",
    "    get_dsc_activations_cell,\n",
    "    activation_function_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "width_fraction = 1.0\n",
    "show_contours = True\n",
    "num_levels = 10\n",
    "contour_fig, contour_handles = pf.plot_group_iso_contours(cont_analysis, num_levels, show_contours, text_width, width_fraction, dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Subspace-LCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lca_subspace_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca_subspace\"\n",
    "    self.model_name = \"lca_subspace_vh\"\n",
    "    self.display_name = \"Subspace Sparse Coding\"\n",
    "    self.version = \"5x_4_1.0_0.2\"\n",
    "    self.save_info = \"analysis_train\"\n",
    "    self.overwrite_analysis_log = False\n",
    "    self.use_group_activations = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, analyzer = load_analysis(lca_subspace_vh_params())\n",
    "neuron_weights = [analyzer.bf_stats[\"basis_functions\"][idx]\n",
    "    for idx in range(len(analyzer.bf_stats[\"basis_functions\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following is a somewhat arbitrary way of choosing some target & comparison vectors that should exhibit both exo- and endo-origin curvature\n",
    "\"\"\"\n",
    "\n",
    "num_target = 3\n",
    "num_within_group = 1\n",
    "num_outside_group = 2\n",
    "min_angle = 15\n",
    "\n",
    "\n",
    "def unique(item_list):\n",
    "    seen = set()\n",
    "    unique_list = []\n",
    "    for x in item_list:\n",
    "        if x not in seen:\n",
    "            unique_list.append(x)\n",
    "            seen.add(x)\n",
    "    return unique_list\n",
    "\n",
    "\n",
    "angle_matrix = iso_data.get_vector_angles(neuron_weights)[1]\n",
    "num_below_min = np.count_nonzero(angle_matrix<min_angle) # many angles are -1 or 0\n",
    "sorted_angle_indices = np.stack(np.unravel_index(np.argsort(angle_matrix.ravel()),\n",
    "    angle_matrix.shape), axis=1)[num_below_min:, :]\n",
    "\n",
    "unique_target_indices = unique(sorted_angle_indices[:,0]) # sorted list of unique target indices\n",
    "\n",
    "all_target_group_assignments = [\n",
    "    analyzer.model.module.group_assignments[index]\n",
    "    for index in unique_target_indices] # list of groups for all possible target indices\n",
    "\n",
    "unique_target_group_assignments = unique(all_target_group_assignments) # sorted list of unique groups\n",
    "target_groups = unique_target_group_assignments[:num_target]\n",
    "\n",
    "selected_target_indices = [analyzer.model.module.group_ids[group_number][0]\n",
    "   for group_number in target_groups] # first neuron in each group\n",
    "\n",
    "within_group_indices = [list(analyzer.model.module.group_ids[group_number][1:num_within_group+1])\n",
    "    for group_number in target_groups] # alternate neuron within the same group as target neuron\n",
    "\n",
    "outside_group_indices = []\n",
    "for group_index, group_number in enumerate(target_groups):\n",
    "    sub_list = []\n",
    "    # loop through a sorted list of all possible comparison IDs, pick the first ones that are in a different group\n",
    "    for candidate_comp_index in sorted_angle_indices[selected_target_indices[group_index], :]:\n",
    "        if analyzer.model.module.group_assignments[candidate_comp_index] != group_number:\n",
    "            sub_list.append(candidate_comp_index)\n",
    "    outside_group_indices.append(sub_list[:num_outside_group])\n",
    "\n",
    "selected_comparison_indices = []\n",
    "selected_comparison_indices = [within_group_indices[group_index]+outside_group_indices[group_index]\n",
    "    for group_index in range(len(target_groups))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cont_analysis = dict()\n",
    "\n",
    "cont_analysis[\"target_neuron_ids\"] = selected_target_indices\n",
    "cont_analysis[\"comparison_neuron_ids\"] = selected_comparison_indices\n",
    "cont_analysis[\"num_comparisons\"] = num_within_group+num_outside_group\n",
    "cont_analysis[\"min_angle\"] = min_angle\n",
    "cont_analysis[\"x_range\"] = (-2.0, 2.0)\n",
    "cont_analysis[\"y_range\"] = (-2.0, 2.0)\n",
    "cont_analysis[\"num_images\"] = int(30**2)\n",
    "cont_analysis[\"image_scale\"] = 33 # norm of training data\n",
    "\n",
    "\n",
    "iso_vectors = iso_data.compute_specified_vectors(\n",
    "    neuron_weights,\n",
    "    cont_analysis[\"target_neuron_ids\"],\n",
    "    cont_analysis[\"comparison_neuron_ids\"])\n",
    "cont_analysis[\"comparison_target_vectors\"] = iso_vectors[0]\n",
    "cont_analysis[\"comparison_vectors\"] = iso_vectors[1]\n",
    "\n",
    "cont_analysis[\"target_vectors\"] = cont_analysis[\"comparison_target_vectors\"]\n",
    "\n",
    "cont_analysis[\"contour_dataset\"], datapoints = iso_data.get_contour_dataset(\n",
    "    cont_analysis[\"target_vectors\"],\n",
    "    cont_analysis[\"comparison_vectors\"],\n",
    "    cont_analysis[\"x_range\"],\n",
    "    cont_analysis[\"y_range\"],\n",
    "    cont_analysis[\"num_images\"],\n",
    "    cont_analysis[\"image_scale\"])\n",
    "\n",
    "activation_function_kwargs = {\n",
    "    \"batch_size\": 100,\n",
    "    \"activation_operation\": analyzer.model.get_reshaped_group_activity}\n",
    "\n",
    "cont_analysis[\"activations\"] = model_funcs.get_normalized_activations(\n",
    "    analyzer,\n",
    "    cont_analysis[\"target_neuron_ids\"],\n",
    "    datapoints,\n",
    "    get_dsc_activations_cell,\n",
    "    activation_function_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width_fraction = 1.0\n",
    "show_contours = True\n",
    "num_levels = 10\n",
    "contour_fig, contour_handles = pf.plot_group_iso_contours(cont_analysis, num_levels, show_contours, text_width, width_fraction, dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_shape = [16, 16]\n",
    "\n",
    "lamb_activation = lambda x : tf.identity(x) # linear\n",
    "#lamb_activation = lambda x : tf.nn.relu(0, x) # ReLU\n",
    "#lamb_activation = lambda x : tf.math.sigmoid(x) # Sigmoid\n",
    "#lamb_activation = lambda x : x / tf.reduce_sum(tf.square(x), axis=1, keepdims=True) # div norm\n",
    "\n",
    "## TODO: energy model defined this way will only have a single output - so the current code can't handle it\n",
    "### We will want to set up something like what is done in lca_subspace.get_reshaped_group_activity\n",
    "#lamb_activation = lambda x : tf.reduce_sum(tf.square(x), axis=1, keepdims=True) # energy model\n",
    "\n",
    "neuron_weights = np.random.normal(loc=0.0, scale=1.0, size=[np.prod(input_shape), np.prod(input_shape)])\n",
    "\n",
    "lambda_params = dsc.params.param_picker.get_params(\"lambda\")\n",
    "lambda_params.set_data_params(\"synthetic\")\n",
    "lambda_params.batch_size = 1 # TODO: support batches in get_lambda_activations_cell\n",
    "lambda_params.data_shape = [np.prod(input_shape)] # assumes vector inputs (i.e. not convoultional)\n",
    "lambda_params.activation_function = lamb_activation\n",
    "\n",
    "lambda_model = dsc.models.model_picker.get_model(\"lambda\")\n",
    "lambda_model.setup(lambda_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lambda_activations_cell(model, images, neuron, weights):\n",
    "    \"\"\"\n",
    "    Returns the activations from a model for given input images\n",
    "    Parameters:\n",
    "        analyzer [DSC analyzer object] an object from the DeepSparseCoding library\n",
    "        images [np.ndarray] of size NumImages x W x H\n",
    "        neuron [int or vector of ints] that points to the neuron index\n",
    "    Output:\n",
    "        activations [np.ndarray] vector of length len(neuron)\n",
    "    \"\"\"\n",
    "    num_images, width, height = images.shape\n",
    "    images = images.reshape([num_images, width*height]) # vectorize images\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.compat.v1.Session(config=config, graph=model.graph) as sess:\n",
    "        feed_dict = model.get_feed_dict(images[0, ...][None, ...])\n",
    "        sess.run(model.init_op, feed_dict)\n",
    "        sess.graph.finalize()\n",
    "        activations = []\n",
    "        for img_idx in range(num_images):\n",
    "            image = images[img_idx, ...]\n",
    "            feed_dict = model.get_feed_dict(image[None, ...])\n",
    "            feed_dict[model.weight_placeholder] = weights\n",
    "            all_activations = sess.run(model.get_encodings(), feed_dict)\n",
    "            activations.append(all_activations[:, neuron])\n",
    "    return np.stack(activations, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cont_analysis = dict()\n",
    "\n",
    "cont_analysis[\"target_neuron_ids\"] = [0, 1, 2]\n",
    "cont_analysis[\"num_comparisons\"] = 3\n",
    "cont_analysis[\"min_angle\"] = 5\n",
    "cont_analysis[\"x_range\"] = (-2.0, 2.0)\n",
    "cont_analysis[\"y_range\"] = (-2.0, 2.0)\n",
    "cont_analysis[\"num_images\"] = int(30**2)\n",
    "cont_analysis[\"image_scale\"] = 1 # norm of training data\n",
    "\n",
    "iso_vectors = iso_data.compute_comp_vectors(\n",
    "    [neuron_weights[:, weight_idx].reshape(input_shape) for weight_idx in range(np.prod(input_shape))],\n",
    "    cont_analysis[\"target_neuron_ids\"],\n",
    "    cont_analysis[\"min_angle\"],\n",
    "    cont_analysis[\"num_comparisons\"])\n",
    "cont_analysis[\"comparison_neuron_ids\"] = iso_vectors[0]\n",
    "cont_analysis[\"comparison_target_vectors\"] = iso_vectors[1]\n",
    "cont_analysis[\"comparison_vectors\"] = iso_vectors[2]\n",
    "\n",
    "cont_analysis[\"target_vectors\"] = cont_analysis[\"comparison_target_vectors\"]\n",
    "\n",
    "cont_analysis[\"contour_dataset\"], datapoints = iso_data.get_contour_dataset(\n",
    "    cont_analysis[\"target_vectors\"],\n",
    "    cont_analysis[\"comparison_vectors\"],\n",
    "    cont_analysis[\"x_range\"],\n",
    "    cont_analysis[\"y_range\"],\n",
    "    cont_analysis[\"num_images\"],\n",
    "    cont_analysis[\"image_scale\"])\n",
    "\n",
    "activation_function_kwargs = {\"weights\": neuron_weights}\n",
    "\n",
    "cont_analysis[\"activations\"] = model_funcs.get_normalized_activations(\n",
    "    lambda_model,\n",
    "    cont_analysis[\"target_neuron_ids\"],\n",
    "    datapoints,\n",
    "    get_lambda_activations_cell,\n",
    "    activation_function_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width_fraction = 1.0\n",
    "show_contours = True\n",
    "num_levels = 10\n",
    "contour_fig, contour_handles = pf.plot_group_iso_contours(cont_analysis, num_levels, show_contours, text_width, width_fraction, dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
